{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740bf3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install tensorflow transformers tf2onnx onnx onnxruntime numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c533e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb053b",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these parameters based on your hardware and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae60025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "MAX_SEQ_LENGTH = 40\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "# Training Configuration  \n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Output paths\n",
    "MODEL_PATH = \"cerebros_model.keras\"\n",
    "ONNX_PATH = \"cerebros_model.onnx\"\n",
    "\n",
    "print(f\"Config: seq_len={MAX_SEQ_LENGTH}, embed_dim={EMBEDDING_DIM}, hidden={HIDDEN_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2a759",
   "metadata": {},
   "source": [
    "## Load Tokenizer\n",
    "\n",
    "Using HuggingFace SmolLM tokenizer (can substitute any tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer from HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "# Ensure pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "print(f\"Tokenizer loaded: vocab_size={VOCAB_SIZE}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a262998",
   "metadata": {},
   "source": [
    "## Interleaved RoPE (iRoPE) Layer\n",
    "\n",
    "Custom positional encoding used by Cerebros architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a740679",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package='cerebros', name='InterleavedRoPE')\n",
    "class InterleavedRoPE(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Interleaved Rotary Position Embedding (iRoPE)\n",
    "    Applies rotary embeddings with interleaved real/imaginary components.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, max_seq_len=512, base=10000.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Precompute frequencies\n",
    "        half_dim = self.dim // 2\n",
    "        freqs = 1.0 / (self.base ** (tf.range(0, half_dim, dtype=tf.float32) / half_dim))\n",
    "        positions = tf.range(self.max_seq_len, dtype=tf.float32)\n",
    "        angles = tf.einsum('i,j->ij', positions, freqs)\n",
    "        \n",
    "        self.cos_cached = tf.cos(angles)\n",
    "        self.sin_cached = tf.sin(angles)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        cos = self.cos_cached[:seq_len]\n",
    "        sin = self.sin_cached[:seq_len]\n",
    "        \n",
    "        # Split into pairs for rotation\n",
    "        x1 = x[..., 0::2]\n",
    "        x2 = x[..., 1::2]\n",
    "        \n",
    "        # Apply rotation\n",
    "        rotated_x1 = x1 * cos - x2 * sin\n",
    "        rotated_x2 = x1 * sin + x2 * cos\n",
    "        \n",
    "        # Interleave back\n",
    "        result = tf.stack([rotated_x1, rotated_x2], axis=-1)\n",
    "        result = tf.reshape(result, tf.shape(x))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'dim': self.dim,\n",
    "            'max_seq_len': self.max_seq_len,\n",
    "            'base': self.base\n",
    "        })\n",
    "        return config\n",
    "\n",
    "print(\"âœ“ InterleavedRoPE layer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471c77c",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "A simple transformer-style architecture with:\n",
    "- Token embeddings\n",
    "- iRoPE positional encoding\n",
    "- Dense layers\n",
    "- Vocabulary prediction head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ae343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cerebros_model(vocab_size, max_seq_len, embed_dim, hidden_dim, num_layers):\n",
    "    \"\"\"Build a Cerebros-style LLM model.\"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = tf.keras.Input(shape=(max_seq_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    \n",
    "    # Token embedding\n",
    "    x = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        name=\"token_embedding\"\n",
    "    )(inputs)\n",
    "    \n",
    "    # Apply iRoPE positional encoding\n",
    "    x = InterleavedRoPE(dim=embed_dim, max_seq_len=max_seq_len, name=\"irope\")(x)\n",
    "    \n",
    "    # Transformer-style processing\n",
    "    for i in range(num_layers):\n",
    "        # Self-attention approximation via dense layers\n",
    "        residual = x\n",
    "        x = tf.keras.layers.LayerNormalization(name=f\"ln_{i}\")(x)\n",
    "        x = tf.keras.layers.Dense(hidden_dim, activation='gelu', name=f\"dense_{i}_1\")(x)\n",
    "        x = tf.keras.layers.Dense(embed_dim, name=f\"dense_{i}_2\")(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        x = x + residual\n",
    "    \n",
    "    # Final normalization\n",
    "    x = tf.keras.layers.LayerNormalization(name=\"final_ln\")(x)\n",
    "    \n",
    "    # Pool sequence (take last position or average)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(name=\"pool\")(x)\n",
    "    \n",
    "    # Output projection to vocabulary\n",
    "    outputs = tf.keras.layers.Dense(vocab_size, name=\"lm_head\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"CerebrosLLM\")\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_cerebros_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_seq_len=MAX_SEQ_LENGTH,\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ce4e48",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "\n",
    "Using sample text for demonstration. Replace with your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e34e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training text (replace with your dataset)\n",
    "SAMPLE_TEXTS = [\n",
    "    \"In the beginning God created the heavens and the earth.\",\n",
    "    \"The earth was without form and void, and darkness was over the face of the deep.\",\n",
    "    \"And the Spirit of God was hovering over the face of the waters.\",\n",
    "    \"And God said, Let there be light, and there was light.\",\n",
    "    \"And God saw that the light was good. And God separated the light from the darkness.\",\n",
    "    \"God called the light Day, and the darkness he called Night.\",\n",
    "    \"And there was evening and there was morning, the first day.\",\n",
    "    \"And God said, Let there be an expanse in the midst of the waters.\",\n",
    "    \"And let it separate the waters from the waters.\",\n",
    "    \"And God made the expanse and separated the waters.\",\n",
    "]\n",
    "\n",
    "def prepare_training_data(texts, tokenizer, max_seq_len):\n",
    "    \"\"\"Prepare training data: input sequences and next-token labels.\"\"\"\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        # Create sliding window samples\n",
    "        for i in range(len(tokens) - 1):\n",
    "            # Input: tokens up to position i, padded to max_seq_len\n",
    "            input_seq = tokens[:i+1]\n",
    "            if len(input_seq) < max_seq_len:\n",
    "                input_seq = input_seq + [tokenizer.pad_token_id] * (max_seq_len - len(input_seq))\n",
    "            else:\n",
    "                input_seq = input_seq[-max_seq_len:]\n",
    "            \n",
    "            # Label: next token\n",
    "            next_token = tokens[min(i+1, len(tokens)-1)]\n",
    "            \n",
    "            input_ids_list.append(input_seq)\n",
    "            labels_list.append(next_token)\n",
    "    \n",
    "    return np.array(input_ids_list, dtype=np.int32), np.array(labels_list, dtype=np.int32)\n",
    "\n",
    "# Prepare data\n",
    "X_train, y_train = prepare_training_data(SAMPLE_TEXTS, tokenizer, MAX_SEQ_LENGTH)\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Input shape: {X_train.shape}\")\n",
    "print(f\"Labels shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e7260",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=LEARNING_RATE),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(f\"\\nðŸ‹ï¸ Training for {EPOCHS} epochs...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"Final loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final accuracy: {history.history['accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef13feb3",
   "metadata": {},
   "source": [
    "## Save Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Keras model\n",
    "model.save(MODEL_PATH)\n",
    "print(f\"âœ… Keras model saved to: {MODEL_PATH}\")\n",
    "print(f\"   Size: {os.path.getsize(MODEL_PATH) / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea0633",
   "metadata": {},
   "source": [
    "## Convert to ONNX\n",
    "\n",
    "Export the trained model to ONNX format for cross-platform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b574d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "print(\"ðŸ”„ Converting to ONNX format...\")\n",
    "\n",
    "# Define input signature\n",
    "input_signature = [\n",
    "    tf.TensorSpec(shape=(None, MAX_SEQ_LENGTH), dtype=tf.int32, name=\"input_ids\")\n",
    "]\n",
    "\n",
    "# Convert to ONNX\n",
    "model_proto, _ = tf2onnx.convert.from_keras(\n",
    "    model,\n",
    "    input_signature=input_signature,\n",
    "    opset=14,\n",
    "    output_path=ONNX_PATH\n",
    ")\n",
    "\n",
    "print(f\"âœ… ONNX model saved to: {ONNX_PATH}\")\n",
    "print(f\"   Size: {os.path.getsize(ONNX_PATH) / 1024:.1f} KB\")\n",
    "\n",
    "# Verify the model\n",
    "onnx_model = onnx.load(ONNX_PATH)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(f\"âœ… ONNX verification passed!\")\n",
    "\n",
    "# Print model info\n",
    "print(f\"\\nðŸ“Š ONNX Model Info:\")\n",
    "print(f\"   IR Version: {onnx_model.ir_version}\")\n",
    "print(f\"   Opset Version: {onnx_model.opset_import[0].version}\")\n",
    "\n",
    "print(f\"\\nðŸ“¥ Inputs:\")\n",
    "for inp in onnx_model.graph.input:\n",
    "    shape = [d.dim_value if d.dim_value else 'batch' for d in inp.type.tensor_type.shape.dim]\n",
    "    print(f\"   {inp.name}: {shape}\")\n",
    "\n",
    "print(f\"\\nðŸ“¤ Outputs:\")\n",
    "for out in onnx_model.graph.output:\n",
    "    shape = [d.dim_value if d.dim_value else 'batch' for d in out.type.tensor_type.shape.dim]\n",
    "    print(f\"   {out.name}: {shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6b700",
   "metadata": {},
   "source": [
    "## Test ONNX Inference\n",
    "\n",
    "Verify the ONNX model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dfad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "print(\"ðŸ§ª Testing ONNX inference...\")\n",
    "\n",
    "# Create ONNX Runtime session\n",
    "ort_session = ort.InferenceSession(ONNX_PATH)\n",
    "\n",
    "# Get input/output names\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "output_name = ort_session.get_outputs()[0].name\n",
    "\n",
    "print(f\"   Input name: {input_name}\")\n",
    "print(f\"   Output name: {output_name}\")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"In the beginning\"\n",
    "test_tokens = tokenizer.encode(test_prompt, add_special_tokens=False)\n",
    "\n",
    "# Pad to MAX_SEQ_LENGTH\n",
    "if len(test_tokens) < MAX_SEQ_LENGTH:\n",
    "    test_tokens = test_tokens + [tokenizer.pad_token_id] * (MAX_SEQ_LENGTH - len(test_tokens))\n",
    "\n",
    "test_input = np.array([test_tokens], dtype=np.int32)\n",
    "print(f\"\\nðŸ“¥ Test input: '{test_prompt}'\")\n",
    "print(f\"   Token IDs: {test_tokens[:10]}...\")\n",
    "\n",
    "# Run ONNX inference\n",
    "onnx_output = ort_session.run([output_name], {input_name: test_input})[0]\n",
    "print(f\"\\nðŸ“¤ ONNX output shape: {onnx_output.shape}\")\n",
    "\n",
    "# Get top predictions\n",
    "probs = tf.nn.softmax(onnx_output[0]).numpy()\n",
    "top_k = 5\n",
    "top_indices = np.argsort(probs)[-top_k:][::-1]\n",
    "\n",
    "print(f\"\\nðŸ” Top {top_k} predicted next tokens:\")\n",
    "for idx in top_indices:\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"   {repr(token)}: {probs[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad13a4",
   "metadata": {},
   "source": [
    "## Compare Keras vs ONNX Outputs\n",
    "\n",
    "Verify the ONNX model produces equivalent results to Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Keras inference\n",
    "keras_output = model(tf.constant(test_input), training=False).numpy()\n",
    "\n",
    "# Compare\n",
    "max_diff = np.max(np.abs(onnx_output - keras_output))\n",
    "mean_diff = np.mean(np.abs(onnx_output - keras_output))\n",
    "\n",
    "print(\"ðŸ“Š Keras vs ONNX Comparison:\")\n",
    "print(f\"   Max absolute difference: {max_diff:.2e}\")\n",
    "print(f\"   Mean absolute difference: {mean_diff:.2e}\")\n",
    "\n",
    "if max_diff < 1e-4:\n",
    "    print(\"\\nâœ… ONNX outputs match Keras (within tolerance)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Some numerical differences (often acceptable for inference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa42ec",
   "metadata": {},
   "source": [
    "## Text Generation (Optional)\n",
    "\n",
    "Simple autoregressive generation using the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b82bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=20, temperature=0.7, top_k=50):\n",
    "    \"\"\"Generate text using the ONNX model.\"\"\"\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Prepare input\n",
    "        input_tokens = tokens[-MAX_SEQ_LENGTH:]\n",
    "        if len(input_tokens) < MAX_SEQ_LENGTH:\n",
    "            input_tokens = input_tokens + [tokenizer.pad_token_id] * (MAX_SEQ_LENGTH - len(input_tokens))\n",
    "        \n",
    "        input_array = np.array([input_tokens], dtype=np.int32)\n",
    "        \n",
    "        # Run inference\n",
    "        logits = ort_session.run([output_name], {input_name: input_array})[0][0]\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Top-k sampling\n",
    "        top_k_indices = np.argsort(logits)[-top_k:]\n",
    "        top_k_logits = logits[top_k_indices]\n",
    "        probs = tf.nn.softmax(top_k_logits).numpy()\n",
    "        \n",
    "        # Sample\n",
    "        next_token_idx = np.random.choice(len(top_k_indices), p=probs)\n",
    "        next_token = top_k_indices[next_token_idx]\n",
    "        \n",
    "        tokens.append(int(next_token))\n",
    "        \n",
    "        # Stop at EOS\n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "# Generate samples\n",
    "print(\"ðŸ”® Generation Samples:\\n\")\n",
    "\n",
    "prompts = [\n",
    "    \"In the beginning\",\n",
    "    \"And God said\",\n",
    "    \"The earth was\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(prompt, max_new_tokens=15, temperature=0.8)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Output: '{generated}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269996b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Files Created\n",
    "- `cerebros_model.keras` - Trained Keras model\n",
    "- `cerebros_model.onnx` - ONNX export for cross-platform inference\n",
    "\n",
    "### Usage in Other Languages\n",
    "\n",
    "**Python (ONNX Runtime):**\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "session = ort.InferenceSession(\"cerebros_model.onnx\")\n",
    "output = session.run([\"output\"], {\"input_ids\": token_ids})\n",
    "```\n",
    "\n",
    "**Elixir (via Ortex):**\n",
    "```elixir\n",
    "{:ok, model} = Ortex.load(\"cerebros_model.onnx\")\n",
    "input = Nx.tensor([[token_ids...]], type: :s32)\n",
    "{output} = Ortex.run(model, input)\n",
    "```\n",
    "\n",
    "**JavaScript (ONNX.js):**\n",
    "```javascript\n",
    "const session = await ort.InferenceSession.create('cerebros_model.onnx');\n",
    "const output = await session.run({input_ids: tensorInput});\n",
    "```\n",
    "\n",
    "**C++ (ONNX Runtime C API):**\n",
    "```cpp\n",
    "Ort::Session session(env, \"cerebros_model.onnx\", session_options);\n",
    "auto output = session.Run(...);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03537aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… COMPLETE: Cerebros LLM Training â†’ ONNX Export\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“ Output Files:\")\n",
    "print(f\"   {MODEL_PATH} ({os.path.getsize(MODEL_PATH)/1024:.1f} KB)\")\n",
    "print(f\"   {ONNX_PATH} ({os.path.getsize(ONNX_PATH)/1024:.1f} KB)\")\n",
    "print(f\"\\nðŸ“Š Model Config:\")\n",
    "print(f\"   Vocabulary: {VOCAB_SIZE:,}\")\n",
    "print(f\"   Sequence Length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"   Embedding Dim: {EMBEDDING_DIM}\")\n",
    "print(f\"   Parameters: {model.count_params():,}\")\n",
    "print(f\"\\nðŸŽ¯ ONNX Input: input_ids [batch, {MAX_SEQ_LENGTH}] int32\")\n",
    "print(f\"ðŸŽ¯ ONNX Output: logits [batch, {VOCAB_SIZE}] float32\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
