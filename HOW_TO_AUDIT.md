# HOW TO AUDIT THUNDERLINE

> **Created**: October 25, 2025  
> **Purpose**: Prevent architectural misunderstandings through systematic codebase auditing  
> **Context**: This methodology was developed after a GitHub search-based audit incorrectly classified ThunderBolt as "experimental" when it contains production saga infrastructure.

---

## ðŸš¨ **THE GOLDEN RULE**

**NEVER audit using GitHub search alone.** GitHub search is useful for discovery but **cannot determine production status, resource counts, or architectural boundaries.**

### Why GitHub Search Fails

âŒ **GitHub search shows**:
- File names (misleading if "demo" or "example" in path)
- Line counts (doesn't indicate production readiness)
- Last commit dates (doesn't reflect usage patterns)

âœ… **File-by-file audit reveals**:
- Actual integration with other domains
- Telemetry and error handling completeness
- Test coverage and production safeguards
- Cross-references in running code
- Supervision tree placement

---

## ðŸ“‹ **AUDIT METHODOLOGY: THE RIGHT WAY**

### Phase 1: Establish Baseline (15 minutes)

#### Step 1: Read the Catalog First
```bash
cat THUNDERLINE_DOMAIN_CATALOG.md
```

**Extract:**
- Current domain list
- Documented resource counts per domain
- Known production vs experimental classifications
- Last update date

**Create checklist:**
```
Domain              Catalog Count    Audit Count    Status
ThunderBlock        29               ?              ?
ThunderBolt         34               ?              ?
ThunderCrown        7                ?              ?
...
```

#### Step 2: List All Domain Directories
```bash
ls -1 lib/thunderline/ | grep -v ".ex$"
```

**Compare against catalog:**
- Missing domains = "ghost domains" (undocumented)
- Extra catalog entries = removed/renamed domains
- Document ALL discrepancies

---

### Phase 2: Count Resources Systematically (30 minutes)

#### For Each Documented Domain:

**Step 1: Count Ash Resources**
```bash
# Count .ex files in resources/ subdirectory
find lib/thunderline/DOMAIN/resources -name "*.ex" -type f 2>/dev/null | wc -l
```

**Step 2: List All Resources**
```bash
# Get actual resource names for verification
find lib/thunderline/DOMAIN/resources -name "*.ex" -type f 2>/dev/null
```

**Step 3: Document in Audit**
```markdown
### ThunderBolt
**Catalog**: 34 resources
**Actual**: [run command and record]
**Delta**: [calculate difference]
**Files**: [list actual .ex files found]
```

#### For Ghost Domains (Not in Catalog):

**Step 1: Identify Structure**
```bash
# Check if it has resources/ directory
ls -la lib/thunderline/DOMAIN/

# Count any .ex files
find lib/thunderline/DOMAIN -name "*.ex" -type f 2>/dev/null | wc -l
```

**Step 2: Determine Purpose**
```bash
# Read main module file if exists
cat lib/thunderline/DOMAIN/domain.ex 2>/dev/null || \
cat lib/thunderline/DOMAIN/DOMAIN.ex 2>/dev/null

# Check for README
cat lib/thunderline/DOMAIN/README.md 2>/dev/null
```

**Step 3: Classify Domain Type**
- **Operational Domain**: Has `resources/` directory with Ash resources
- **Utility Domain**: Has `.ex` files but no `resources/` directory
- **Legacy Domain**: Empty or only stub files

---

### Phase 3: Assess Production Status (45 minutes)

**CRITICAL**: This is where GitHub search fails catastrophically. You MUST read actual code.

#### For Each Domain (Priority: Largest First):

**Step 1: Read Main Domain File**
```bash
cat lib/thunderline/DOMAIN/domain.ex
```

**Look for:**
- `use Ash.Domain` (operational domain)
- Resource registration list
- Domain configuration

**Step 2: Check for Saga Infrastructure**
```bash
# List saga files
find lib/thunderline/DOMAIN -path "*/sagas/*.ex" -type f 2>/dev/null

# Count lines in each saga (complexity indicator)
wc -l lib/thunderline/DOMAIN/sagas/*.ex 2>/dev/null
```

**Production indicators:**
- Multiple saga files (not just one demo)
- Large line counts (>100 lines = substantial logic)
- Names like "UserProvisioning", "ModelActivation" (real workflows)
- Integration with multiple domains (cross-domain orchestration)

**Step 3: Check Cross-Domain References**
```bash
# Find references to this domain in OTHER domains
grep -r "alias Thunderline\.DOMAIN\." lib/thunderline/ --include="*.ex" | \
  grep -v "lib/thunderline/DOMAIN/" | \
  head -20
```

**Production proof:**
- Multiple domains import this domain's modules
- Found in supervision trees (supervisor.ex files)
- Referenced in event handlers
- Used in action changes/preparations

**Step 4: Check Test Coverage**
```bash
# Find test files for this domain
find test -path "*DOMAIN*" -name "*.exs" -type f 2>/dev/null | wc -l

# Check test complexity
wc -l test/**/DOMAIN/**/*.exs 2>/dev/null
```

**Production indicator:**
- Substantial test files (not just smoke tests)
- Integration tests with other domains
- Saga compensation testing

**Step 5: Check Telemetry Integration**
```bash
# Search for telemetry events
grep -r "telemetry.execute" lib/thunderline/DOMAIN/ --include="*.ex" | wc -l
grep -r ":telemetry.span" lib/thunderline/DOMAIN/ --include="*.ex" | wc -l
```

**Production indicator:**
- Comprehensive telemetry coverage
- Error telemetry and monitoring
- Performance tracking

---

### Phase 4: Document Findings (30 minutes)

#### Create Audit Document

**File**: `CODEBASE_AUDIT_YYYY_MM.md`

**Structure:**
```markdown
# Thunderline Codebase Audit - [Month Year]

> **Auditor**: [Your Name]  
> **Date**: [Today's Date]  
> **Method**: File-by-file systematic audit (NOT GitHub search)  
> **Baseline**: THUNDERLINE_DOMAIN_CATALOG.md dated [catalog date]

## Executive Summary

- **Domains Audited**: X
- **Resources Counted**: X (catalog claimed Y)
- **Discrepancies Found**: X
- **Ghost Domains Discovered**: X
- **Production Status Updates**: X

## Methodology

[Describe your process - reference this guide]

## Findings by Domain

### ThunderBlock
**Catalog Count**: 23 resources
**Actual Count**: 29 resources
**Delta**: +6 resources (UNDERCOUNTED)

**Missing from Catalog**:
- [List the 6 resources found but not documented]

**Production Status**: âœ… VERIFIED
**Evidence**:
- [Cross-domain references found]
- [Test coverage details]
- [Telemetry integration points]

[Repeat for each domain]

## Ghost Domains

### ThunderChief (NEW - Not in Catalog)
**Path**: lib/thunderline/thunderchief/
**Type**: Utility Domain (no Ash resources)
**Files**: 4 .ex files
**Purpose**: [Determined from code reading]
**Recommendation**: ADD to catalog as utility domain

[Repeat for each ghost domain]

## Recommendations

1. **Update Catalog**: [List specific corrections needed]
2. **Clarify Status**: [List domains needing production/experimental classification]
3. **Add Domains**: [List ghost domains to document]
4. **Remove Domains**: [List legacy domains to archive]

## Audit Verification

**Commands Used**:
```bash
[List all commands used for verification]
```

**Files Read**:
- [List key files examined]

**Cross-References Checked**:
- [List grep commands used]
```

---

## ðŸŽ¯ **PRODUCTION STATUS DETERMINATION**

### âœ… Domain is PRODUCTION if:

1. **Multiple Substantial Files** (not just demos)
   - Saga files >100 lines
   - Multiple resources with complete CRUD
   - Complex business logic

2. **Cross-Domain Integration**
   - Referenced in 3+ other domains
   - Part of supervision tree
   - Event bus integration

3. **Error Handling & Telemetry**
   - Comprehensive error handling
   - Telemetry instrumentation
   - Logging and monitoring

4. **Test Coverage**
   - Integration tests exist
   - Not just unit tests
   - Compensation/rollback tests for sagas

5. **Active Usage Evidence**
   - Recent meaningful commits (not just formatting)
   - Referenced in documentation
   - Part of system architecture diagrams

### âš ï¸ Domain is EXPERIMENTAL if:

1. **Proof of Concept Code**
   - Files named "demo", "example", "poc"
   - Minimal error handling
   - No test coverage

2. **Isolated/Unused**
   - No cross-domain references
   - Not in supervision tree
   - No event integration

3. **Documentation Gaps**
   - Not mentioned in catalog
   - No purpose documentation
   - No integration guides

### ðŸ—‘ï¸ Domain is LEGACY if:

1. **Deprecated/Stub Code**
   - Empty or placeholder files only
   - Comments saying "TODO" or "DEPRECATED"
   - No recent commits (>1 year)

2. **Replaced by Other Domains**
   - Documentation mentions consolidation
   - Code moved to other domains
   - Migration guides exist

---

## ðŸš« **ANTI-PATTERNS: What NOT To Do**

### âŒ The GitHub Search Mistake

**What Happened**: El Tigre's audit used GitHub search to find "saga" files, saw ThunderBolt had sagas, but couldn't determine from search results that they were production-grade. Incorrectly classified as "experimental orchestration to archive."

**Why It Failed**:
- GitHub search shows file names, not code quality
- Presence of "demo_job.ex" in ThunderChief looked similar to saga files
- No way to see cross-domain integration from search
- Can't assess line count complexity from search results
- Missing context of supervision trees and event integration

**Lesson**: GitHub search is for DISCOVERY only, never for CLASSIFICATION.

---

### âŒ The Line Count Fallacy

**Mistake**: "This file is small (50 lines), so it must be experimental."

**Reality**: Small files can be:
- Highly polished production code (concise is better)
- Gateway modules that delegate to other systems
- Configuration files for complex systems

**Correct Approach**: Check WHAT the file does and WHERE it's used, not just size.

---

### âŒ The Name Bias Error

**Mistake**: "This directory has 'demo' in the name, so everything is experimental."

**Reality**: `demo_job.ex` might be:
- Example code for documentation âœ… experimental
- Demo for external showcase âœ… experimental
- Actual job that demonstrates system capability to users âŒ PRODUCTION

**Correct Approach**: Read the file and check if it's called by production code.

---

### âŒ The Isolation Assumption

**Mistake**: "This domain isn't imported much, so it's not production."

**Reality**:
- Foundation domains (Block, Flow) are imported everywhere
- Specialized domains (Crown, Grid) have fewer imports but are critical
- Utility domains (Chief, Forge) are only imported where needed

**Correct Approach**: Check import frequency RELATIVE to domain purpose, not absolute count.

---

### âŒ The Timestamp Trap

**Mistake**: "Last commit was months ago, so it's abandoned."

**Reality**:
- Stable production code doesn't need frequent changes
- Major refactors happen in batches
- Security updates might be the only commits

**Correct Approach**: Check if code is REFERENCED by recently updated code, not just when domain itself was edited.

---

## ðŸ“Š **AUDIT CHECKLIST**

Use this checklist for every audit:

### Pre-Audit
- [ ] Read current THUNDERLINE_DOMAIN_CATALOG.md
- [ ] Extract baseline resource counts
- [ ] List all documented domains
- [ ] Check catalog last update date

### Resource Counting
- [ ] List all domain directories in lib/thunderline/
- [ ] For each domain: count .ex files in resources/
- [ ] For each domain: list actual resource files
- [ ] Compare actual vs catalog counts
- [ ] Document all discrepancies
- [ ] Identify ghost domains (not in catalog)

### Production Status Assessment
- [ ] Read domain.ex for each operational domain
- [ ] Check for saga infrastructure (if applicable)
- [ ] Count lines in saga files (complexity indicator)
- [ ] Search for cross-domain references (grep)
- [ ] Check test coverage (find test files)
- [ ] Verify telemetry integration (grep)
- [ ] Check supervision tree inclusion
- [ ] Review recent commit activity (context matters)

### Classification
- [ ] Mark each domain: PRODUCTION / EXPERIMENTAL / LEGACY
- [ ] Document evidence for each classification
- [ ] Flag any unclear cases for team review
- [ ] Verify against running application (if possible)

### Documentation
- [ ] Create audit document with findings
- [ ] List all resource count corrections needed
- [ ] List all ghost domains found
- [ ] List all production status clarifications
- [ ] Provide specific recommendations
- [ ] Include verification commands used

### Validation
- [ ] Have another team member spot-check findings
- [ ] Run application to verify critical domains work
- [ ] Check that saga system actually runs
- [ ] Verify test suite passes

---

## ðŸ› ï¸ **AUDIT TOOLBOX**

### Essential Commands

```bash
# Count resources in a domain
find lib/thunderline/DOMAIN/resources -name "*.ex" -type f 2>/dev/null | wc -l

# List all resources with full paths
find lib/thunderline/DOMAIN/resources -name "*.ex" -type f 2>/dev/null

# Find all domains (including ghosts)
ls -1 lib/thunderline/ | grep -v ".ex$"

# Check cross-domain references
grep -r "alias Thunderline\.DOMAIN\." lib/thunderline/ --include="*.ex" | \
  grep -v "lib/thunderline/DOMAIN/" | wc -l

# Find saga files
find lib/thunderline/DOMAIN -path "*/sagas/*.ex" -type f 2>/dev/null

# Count saga complexity
wc -l lib/thunderline/DOMAIN/sagas/*.ex 2>/dev/null

# Check test coverage
find test -path "*DOMAIN*" -name "*.exs" -type f 2>/dev/null | wc -l

# Search for telemetry
grep -r "telemetry" lib/thunderline/DOMAIN/ --include="*.ex" | wc -l

# Check supervision tree
grep -r "Supervisor.child_spec" lib/thunderline/ --include="*.ex" | \
  grep "DOMAIN"

# Find recent activity (last 90 days)
git log --since="90 days ago" --oneline lib/thunderline/DOMAIN/ | wc -l
```

### Batch Audit Script

Save as `audit_all_domains.sh`:

```bash
#!/bin/bash

echo "THUNDERLINE AUDIT REPORT"
echo "Date: $(date)"
echo "========================="
echo ""

for domain in lib/thunderline/*/; do
    domain_name=$(basename "$domain")
    
    # Skip if it's a file, not a directory
    [ -d "$domain" ] || continue
    
    echo "### $domain_name"
    
    # Count resources
    resource_count=$(find "$domain/resources" -name "*.ex" -type f 2>/dev/null | wc -l)
    echo "Resources: $resource_count"
    
    # Count sagas
    saga_count=$(find "$domain" -path "*/sagas/*.ex" -type f 2>/dev/null | wc -l)
    echo "Sagas: $saga_count"
    
    # Count total .ex files
    total_files=$(find "$domain" -name "*.ex" -type f 2>/dev/null | wc -l)
    echo "Total .ex files: $total_files"
    
    # Cross-domain references
    refs=$(grep -r "alias Thunderline\.$domain_name\." lib/thunderline/ --include="*.ex" 2>/dev/null | \
           grep -v "lib/thunderline/$domain_name/" | wc -l)
    echo "Cross-domain refs: $refs"
    
    # Test files
    tests=$(find test -path "*$domain_name*" -name "*.exs" -type f 2>/dev/null | wc -l)
    echo "Test files: $tests"
    
    echo ""
done
```

Run with:
```bash
chmod +x audit_all_domains.sh
./audit_all_domains.sh > audit_report_$(date +%Y%m%d).txt
```

---

## ðŸ“ **EXAMPLE: Correct vs Incorrect Audit**

### âŒ INCORRECT AUDIT (GitHub Search Method)

```markdown
## ThunderBolt Audit

**Method**: GitHub search for "saga" and "orchestration"

**Findings**:
- Found files with "saga" in name
- Directory contains "demo" jobs
- Some experimental-looking orchestration code

**Conclusion**: EXPERIMENTAL - Should archive

**Evidence**: 
- GitHub search results show saga files
- Presence of demo_job.ex
```

**PROBLEMS**:
- No file reading (can't assess quality)
- Name-based classification (demo â‰  experimental)
- No cross-domain check (missing production integration)
- No line count (247-line saga is substantial)
- No telemetry check (missing monitoring integration)

---

### âœ… CORRECT AUDIT (File-by-File Method)

```markdown
## ThunderBolt Audit

**Method**: File-by-file reading + cross-reference checking

**Resource Count**:
- Catalog: 30 resources
- Actual: 34 resources
- Delta: +4 (undercounted)

**Saga Infrastructure Found**:
1. `sagas/base.ex` (142 lines) - Framework with telemetry
2. `sagas/cerebros_nas_saga.ex` (247 lines, 9 steps) - ML pipeline
3. `sagas/user_provisioning_saga.ex` (158 lines, 7 steps) - Cross-domain
4. `sagas/upm_activation_saga.ex` - Model activation
5. `sagas/registry.ex` - Process tracking
6. `sagas/supervisor.ex` - Fault tolerance

**Cross-Domain Integration**:
- ThunderGate: 15 references (auth workflows)
- ThunderBlock: 12 references (persistence)
- ThunderFlow: 8 references (event orchestration)
- Referenced in application.ex supervision tree

**Test Coverage**:
- 7 test files covering saga workflows
- Integration tests with other domains
- Compensation/rollback tests present

**Telemetry**:
- 23 telemetry.execute calls
- Full span instrumentation
- Error tracking integrated

**Conclusion**: âœ… PRODUCTION - Battle-tested saga infrastructure

**Evidence**:
- Substantial saga files (>100 lines each)
- Cross-domain orchestration (Gateâ†’Blockâ†’Link)
- Complete error handling and telemetry
- Supervised process architecture
- Active usage in ML pipelines and user onboarding
```

**WHY THIS IS CORRECT**:
- Actually read the code (knows line counts, step counts)
- Checked cross-domain usage (proves production integration)
- Verified supervision (proves system-critical)
- Checked telemetry (proves operational monitoring)
- Assessed complexity (>100 lines = substantial)

---

## ðŸŽ“ **LESSONS LEARNED**

### From the ThunderBolt Incident

**What Went Wrong**:
El Tigre conducted an audit using GitHub search and file names, concluding ThunderBolt was "experimental orchestration" to archive. This was **completely wrong** - ThunderBolt contains production saga infrastructure driving ML pipelines and user onboarding.

**Why It Went Wrong**:
1. **Method**: GitHub search can't assess production readiness
2. **Name Bias**: Presence of "demo" files suggested experimental
3. **No Code Reading**: Didn't see 247-line CerebrosNASSaga complexity
4. **No Cross-Check**: Missed Gateâ†’Blockâ†’Link orchestration
5. **No Telemetry Check**: Missed comprehensive monitoring

**How To Prevent**:
1. **ALWAYS read actual code files** before classification
2. **Check cross-domain references** to prove integration
3. **Count lines in substantial files** (>100 = production-grade)
4. **Verify supervision tree** inclusion
5. **Check telemetry coverage** for operational readiness

### Key Principles

1. **Discovery â‰  Classification**
   - GitHub search is useful for finding files
   - Classification requires reading those files

2. **Names Lie, Code Doesn't**
   - "demo" might be production demonstration code
   - "experimental" might be graduated to production
   - Read the code to know the truth

3. **Integration Proves Production**
   - If 3+ domains import it â†’ production
   - If in supervision tree â†’ production
   - If event bus uses it â†’ production

4. **Telemetry = Operational**
   - Comprehensive telemetry â†’ production monitoring
   - No telemetry â†’ experimental/legacy

5. **Tests Indicate Intent**
   - Integration tests â†’ intended for production
   - Only unit tests â†’ might be experimental
   - No tests â†’ definitely not production-ready

---

## ðŸš€ **AUDIT WORKFLOW SUMMARY**

```
START
  â†“
Read Catalog (baseline)
  â†“
Count Resources (file-by-file)
  â†“
Compare to Catalog (find discrepancies)
  â†“
Identify Ghost Domains
  â†“
For Each Domain:
  â”œâ”€ Read domain.ex
  â”œâ”€ Count saga files (if any)
  â”œâ”€ Check cross-domain refs
  â”œâ”€ Verify test coverage
  â”œâ”€ Check telemetry
  â””â”€ Classify: PRODUCTION / EXPERIMENTAL / LEGACY
  â†“
Document Findings
  â†“
Validate with Team
  â†“
Update Catalog
  â†“
END
```

**Time Estimate**:
- Small codebase (<10 domains): 2-3 hours
- Medium codebase (10-20 domains): 4-6 hours  
- Large codebase (>20 domains): Full day

**Frequency**:
- After major refactors: Immediate
- Regular audits: Quarterly
- Before architecture decisions: Always

---

## ðŸ“ž **GETTING HELP**

If you're unsure about classification:

1. **Read this guide again** - Most answers are here
2. **Check with the team** - Don't guess on production status
3. **Run the application** - See if the domain actually works
4. **Check git history** - Context from recent changes helps
5. **Review tests** - Test intent often reveals production status

**When in doubt, mark as "NEEDS REVIEW" and get team input.**

---

## âœ… **AUDIT QUALITY CHECKLIST**

Before submitting your audit:

- [ ] Used file-by-file method (not GitHub search)
- [ ] Read actual code for all major domains
- [ ] Verified resource counts match actual files
- [ ] Documented ALL discrepancies found
- [ ] Classified production status with evidence
- [ ] Listed commands used for verification
- [ ] Spot-checked findings with another developer
- [ ] Documented ghost domains discovered
- [ ] Provided specific catalog update recommendations
- [ ] Included timeline for corrections

**Sign-off**: [Your Name] - [Date]

---

**Remember**: A good audit takes time. A bad audit (like GitHub search) causes confusion, wasted effort, and potentially archives production code. **Do it right the first time.** ðŸŽ¯
