# Thunderline AI/ML Integration Manifest

**Status**: ‚úÖ Foundation Complete | üöÄ Ready for Development  
**Date**: 2025-11-11  
**Author**: System Architect

---

## üì¶ Delivered Artifacts

### 1. Documentation
- ‚úÖ `docs/MLFLOW_3_MIGRATION_GUIDE.md` - Updated with Pydantic v2 fixes
- ‚úÖ `docs/AI_ML_INTEGRATION_GUIDE.md` - Comprehensive architecture doc
- ‚úÖ `docs/AI_ML_DEVELOPER_QUICKSTART.md` - Developer onboarding
- ‚úÖ `THUNDERLINE_DOMAIN_CATALOG.md` - Updated with new modules
- ‚úÖ This manifest

### 2. Python Components
```
python/nlp_cli.py              # spaCy NLP service (JSON-line protocol)
python/requirements.txt         # Dependencies (spacy, magika)
```

### 3. Elixir Modules

#### ThunderGate (Ingestion)
```elixir
lib/thunderline/thundergate/magika.ex
  - classify_file/1
  - classify_bytes/2
  - Emits: system.ingest.classified
```

#### ThunderFlow (Telemetry)
```elixir
lib/thunderline/thunderbolt/telemetry.ex
  - python_bridge_start/3
  - python_bridge_stop/3
  - python_bridge_error/3
  - ortex_session_load/2
  - ortex_inference_start/2
  - ortex_inference_stop/3
```

#### ThunderBolt (AI/ML)
```elixir
lib/thunderline/thunderbolt/nlp/port.ex
  - NLP.Port - Supervised spaCy bridge
  - analyze/2 - JSON-line protocol
  - Emits: ai.nlp.analyzed

lib/thunderline/thunderbolt/models/keras_onnx.ex
  - Nx.Serving adapter for Ortex
  - load_model/1
  - run_inference/2
  - Emits: ai.ml.run.completed

lib/thunderline/thunderbolt/voxel.ex
  - build/1 - Package artifacts
  - Voxel schema v0
  - Emits: dag.commit
```

---

## üéØ Integration Points

### Event Flow
```
1. ThunderGate.Magika.classify_file/1
   ‚Üì system.ingest.classified
   
2. ThunderBolt.NLP.Port.analyze/2
   ‚Üì ai.nlp.analyzed
   
3. ThunderBolt.Models.KerasONNX.run_inference/2
   ‚Üì ai.ml.run.completed
   
4. ThunderBolt.Voxel.build/1
   ‚Üì dag.commit
   
5. ThunderBlock persists voxel
```

### Supervision Tree
```
Thunderline.Application
  ‚îú‚îÄ ThunderBolt.Serving.KerasONNX (Nx.Serving)
  ‚îî‚îÄ ThunderBolt.NLP.Port (GenServer w/ backoff)
```

---

## üîß Developer Quickstart

### 1. Install Python Dependencies
```bash
cd /home/mo/DEV/Thunderline
source .venv/bin/activate
pip install -r python/requirements.txt
python -m spacy download en_core_web_sm
pip install magika
```

### 2. Test Python CLI
```bash
echo '{"op":"analyze","text":"Apple Inc. hired Tim Cook.","lang":"en"}' | \
  python python/nlp_cli.py
# Expected: {"ok":true,"entities":[...],"tokens":[...]}
```

### 3. Export Keras Model to ONNX
```bash
# In Python:
# import tensorflow as tf
# model = tf.keras.models.load_model("model.keras")
# tf.saved_model.save(model, "exported")
# 
# python -m tf2onnx.convert \
#   --saved-model exported \
#   --output priv/models/model.onnx \
#   --opset 17
```

### 4. Start Elixir Services
```elixir
# In iex -S mix:

# 1. Test Magika
{:ok, result} = ThunderGate.Magika.classify_file("test.pdf")

# 2. Test NLP
{:ok, nlp} = ThunderBolt.NLP.Port.analyze("Google is in California.", %{})

# 3. Test ONNX (after model export)
{:ok, _} = ThunderBolt.Models.KerasONNX.load_model("priv/models/model.onnx")
input = Nx.tensor([[1.0, 2.0, 3.0]])
{:ok, output} = ThunderBolt.Models.KerasONNX.run_inference(input, %{})

# 4. Build voxel
{:ok, voxel} = ThunderBolt.Voxel.build(%{
  classified: result,
  nlp: nlp,
  model_output: output,
  content_fingerprint: "abc123",
  correlation_id: UUID.uuid4()
})
```

---

## üìã Next Steps (Priority Order)

### Phase 1: Validation (Week 1)
- [ ] Test `nlp_cli.py` with production text volumes
- [ ] Export one production Keras model to ONNX
- [ ] Benchmark Ortex inference latency
- [ ] Test voxel persistence in ThunderBlock

### Phase 2: Integration (Week 2)
- [ ] Wire ThunderFlow pipelines: ingest ‚Üí classify ‚Üí NLP ‚Üí ML
- [ ] Add DLQ handling for Python bridge failures
- [ ] Implement voxel storage in ThunderBlock
- [ ] Add correlation/causation ID enforcement

### Phase 3: Observability (Week 3)
- [ ] Add Telemetry dashboard for Python bridge
- [ ] Add Ortex session metrics
- [ ] Implement error classification (transient vs permanent)
- [ ] Add retry policies with exponential backoff

### Phase 4: Production Hardening (Week 4)
- [ ] Add PII redaction in logging
- [ ] Implement voxel retention policies
- [ ] Add model version tracking
- [ ] Load testing: 1000 req/s ‚Üí ML pipeline

---

## üö® Critical Decisions Made

### 1. Python Bridge: CLI JSON vs gRPC
**Decision**: CLI with JSON-line protocol  
**Rationale**: Zero deployment friction, working path today, swappable later  
**Trade-off**: Lower throughput than gRPC, acceptable for Phase 1

### 2. Keras ‚Üí Elixir: ONNX + Ortex
**Decision**: tf2onnx ‚Üí Ortex (not Bumblebee)  
**Rationale**: Portable for arbitrary Keras graphs, Nx.Serving compatibility  
**Trade-off**: Manual conversion step, gains model portability

### 3. Voxel Format: JSON + NPY
**Decision**: Header JSON + artifact binaries  
**Rationale**: Simple, inspectable, DAG-friendly  
**Trade-off**: Not compressed yet, good for Phase 1

### 4. Event Taxonomy: New Categories
**Decision**: Added `ai.nlp.*` and `ai.ml.*` to EVENT_TAXONOMY.md  
**Rationale**: Aligns with domain boundaries (ThunderBolt = AI/ML)  
**Trade-off**: None, extends existing taxonomy cleanly

---

## üîç Testing Strategy

### Unit Tests
```bash
# Python
pytest python/test_nlp_cli.py

# Elixir
mix test test/thunderbolt/nlp/port_test.exs
mix test test/thunderbolt/models/keras_onnx_test.exs
mix test test/thunderbolt/voxel_test.exs
```

### Integration Tests
```bash
# End-to-end pipeline
mix test test/integration/ai_ml_pipeline_test.exs

# Broadway consumers
mix test test/thunderflow/pipelines/nlp_test.exs
```

### Load Tests
```bash
# Ortex inference
mix run bench/ortex_inference_bench.exs

# Python bridge throughput
mix run bench/nlp_port_bench.exs
```

---

## üìä Success Metrics

### Performance Targets
- **Magika classification**: <100ms/file
- **spaCy NLP**: <200ms/doc (avg 1000 tokens)
- **Ortex inference**: <50ms/batch (batch_size=32)
- **Voxel build**: <500ms (all artifacts)
- **End-to-end latency**: <1s (ingest ‚Üí voxel commit)

### Reliability Targets
- **Python bridge uptime**: 99.9%
- **Ortex session crashes**: <0.1%/day
- **Event ordering**: 100% (correlation/causation enforced)
- **Voxel integrity**: 100% (BLAKE3 checksums)

---

## üõ†Ô∏è Tooling

### Development
```bash
# Watch mode for Python
watchmedo auto-restart -d python -p "*.py" -- python python/nlp_cli.py

# Elixir hot reload
iex -S mix phx.server

# ONNX model inspector
pip install netron
netron priv/models/model.onnx
```

### Debugging
```bash
# Python CLI manual test
echo '{"op":"analyze","text":"Test","lang":"en"}' | python python/nlp_cli.py

# Ortex session info
Ortex.info(session)

# Voxel contents
:zlib.gunzip(File.read!("voxels/abc123.voxel")) |> Jason.decode!()
```

---

## üìö Reference Documentation

### Internal
- `EVENT_TAXONOMY.md` - Event naming rules
- `THUNDERLINE_DOMAIN_CATALOG.md` - Module registry
- `copilot-instructions.md` - Core usage rules

### External
- [Ortex Docs](https://hexdocs.pm/ortex)
- [tf2onnx Guide](https://github.com/onnx/tensorflow-onnx)
- [Magika Repo](https://github.com/google/magika)
- [spaCy Models](https://spacy.io/models)

---

## ‚úÖ Acceptance Criteria

**Definition of Done**:
1. ‚úÖ All 7 modules compile without warnings
2. ‚úÖ Python CLI returns valid JSON for 100 test cases
3. ‚úÖ One Keras model runs inference via Ortex
4. ‚úÖ Voxel persists to disk with valid schema
5. ‚úÖ Telemetry events emit for all operations
6. ‚úÖ Integration tests pass (green CI)
7. ‚úÖ Documentation merged to `main`

---

## üéñÔ∏è Sign-Off

**Architect**: Ready for development sprint  
**Delivery**: Foundation modules landed  
**Next**: Team review ‚Üí Phase 1 validation  

**Ship it.** üöÄ
